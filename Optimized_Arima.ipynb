{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX2V8WQDv7HU"
      },
      "outputs": [],
      "source": [
        "#Final Arima+\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import mysql.connector\n",
        "import dotenv\n",
        "from dotenv import dotenv_values\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#reding the table from server-----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "filename = os.path.basename(__file__)\n",
        "path = str(__file__).replace(filename, '')\n",
        "env = dotenv_values(path + '.env')\n",
        "# env = dotenv_values(r'C:\\Users\\satia\\Desktop\\137\\zobale'+'.env')\n",
        "query = \"SELECT * \"\n",
        "query += \" FROM `request_statistics`\"\n",
        "query += \" \"\n",
        "\n",
        "try:\n",
        "    connection = mysql.connector.connect(host=env['DB_HOST'], database=env['DB_DATABASE'], user=env['DB_USERNAME'],\n",
        "                                         password=env['DB_PASSWORD'])\n",
        "\n",
        "#    connection = mysql.connector.connect(host=\"192.168.168.125\", database=\"requestdb\", user=\"requestuser\",\n",
        "                                         #password=\"SxfWL7myDid1JBqS\")\n",
        "except mysql.connector.Error as err:\n",
        "    print('There was an error connecting to the database!!!')\n",
        "    print(err)\n",
        "    exit()\n",
        "cursor = connection.cursor(dictionary=True)\n",
        "cursor.execute(query)\n",
        "last = cursor.fetchall()\n",
        "last = pd.DataFrame(last)\n",
        "# print (dfm)\n",
        "if connection.is_connected():\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "#removing unnecessary columns of the table--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "last = last.drop('statisticsType', axis=1)\n",
        "last = last.drop('subjectID', axis=1)\n",
        "last = last.drop('id', axis=1)\n",
        "last = last.drop('created_at', axis=1)\n",
        "last = last.drop('updated_at', axis=1)\n",
        "last.drop(last.tail(1).index,inplace=True)\n",
        "#_______________________________________________________________________________________________________\n",
        "last = last.drop('time', axis=1)\n",
        "print (last)\n",
        "\n",
        "data=last.copy()\n",
        "# Function to handle outliers/anomalies\n",
        "def iqr_fence(x):\n",
        "    Q1 = x.quantile(0.25)\n",
        "    Q3 = x.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    Lower_Fence = Q1 - (1.5 * IQR)\n",
        "    Upper_Fence = Q3 + (1.5* IQR)\n",
        "    u = max(x[x<Upper_Fence])\n",
        "    l = min(x[x>Lower_Fence])\n",
        "    return [u,l]\n",
        "\n",
        "\n",
        "def handle_outliers(data):\n",
        "\n",
        "    for column in data.columns:\n",
        "        threshold=iqr_fence(data[column])[0]\n",
        "        data[column] = data[column].mask(data[column] > threshold, int(threshold))\n",
        "    return data\n",
        "\n",
        "# Handle outliers/anomalies in the dataset\n",
        "data = handle_outliers(data)\n",
        "print(data)\n",
        "\n",
        "# Define the objective function for hyperopt\n",
        "def arima_objective(params):\n",
        "    p, d, q = params['p'], params['d'], params['q']\n",
        "\n",
        "    try:\n",
        "        # Fit ARIMA model\n",
        "        model = ARIMA(train_data, order=(p, d, q))\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Make one-step ahead predictions\n",
        "        predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1, typ='levels')\n",
        "\n",
        "        # Calculate MSE\n",
        "        mse = mean_squared_error(test_data, predictions)\n",
        "\n",
        "        return {'loss': mse, 'status': STATUS_OK}\n",
        "\n",
        "    except:\n",
        "        return {'loss': np.inf, 'status': STATUS_OK}\n",
        "Prediction_Metrice=[]\n",
        "MAE_Percentage_Error=[]\n",
        "# Iterate over each region column\n",
        "for region in data.columns:\n",
        "    # Get the region data\n",
        "    region_data = data[region].values\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_size = int(len(region_data) * 0.8)\n",
        "    train_data, test_data = region_data[:train_size], region_data[train_size:]\n",
        "\n",
        "    # Define the search space for hyperopt\n",
        "    space = {\n",
        "        'p': hp.choice('p', range(0, 5)),\n",
        "        'd': hp.choice('d', range(0, 5)),\n",
        "        'q': hp.choice('q', range(0, 5))\n",
        "    }\n",
        "\n",
        "    # Run hyperopt optimization\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=arima_objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "    # Get the best parameters\n",
        "    best_p = best['p']\n",
        "    best_d = best['d']\n",
        "    best_q = best['q']\n",
        "\n",
        "    # Fit ARIMA model with best parameters\n",
        "    best_model = ARIMA(train_data, order=(best_p, best_d, best_q))\n",
        "    best_model_fit = best_model.fit()\n",
        "\n",
        "    # Make final predictions with the best model\n",
        "    predictions = best_model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1, typ='levels')\n",
        "\n",
        "    Real_prediction = best_model_fit.predict(start=len(train_data) + len(test_data)+1 , end=len(train_data) + len(test_data)+1 , typ='levels')\n",
        "\n",
        "    Prediction_Metrice.append(Real_prediction)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "    mse = mean_squared_error(test_data, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(test_data, predictions)\n",
        "    r2 = r2_score(test_data, predictions)\n",
        "\n",
        "\n",
        "    MAE_Percentage_Error.append(mae*100/np.mean(data[region][-4:]))\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(\"Region:\", region)\n",
        "    print(\"Best Parameters: p =\", best_p, \"d =\", best_d, \"q =\", best_q)\n",
        "    print(\"Test Actual values:\", test_data)\n",
        "    print(\"Test Predicted values:\", predictions)\n",
        "    print(\"One Step Ahead Value:\", Real_prediction)\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R2:\", r2)\n",
        "    print()\n",
        "\n",
        "print(\"Prediction_Metrice:\", Prediction_Metrice)\n",
        "print(\"MAE_Percentage_Error:\", MAE_Percentage_Error)"
      ]
    }
  ]
}