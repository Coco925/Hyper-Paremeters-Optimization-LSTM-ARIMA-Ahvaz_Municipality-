{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX2V8WQDv7HU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
        "import mysql.connector\n",
        "import dotenv\n",
        "from dotenv import dotenv_values\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#reding the table from server-----------------------------------------------------------------------------------------------------------------\n",
        "#test git\n",
        "filename = os.path.basename(__file__)\n",
        "path = str(__file__).replace(filename, '')\n",
        "env = dotenv_values(path + '.env')\n",
        "# env = dotenv_values(r'C:\\Users\\satia\\Desktop\\137\\zobale'+'.env')\n",
        "query = \"SELECT * \"\n",
        "query += \" FROM `request_statistics`\"\n",
        "query += \" \"\n",
        "\n",
        "try:\n",
        "    connection = mysql.connector.connect(host=env['DB_HOST'], database=env['DB_DATABASE'], user=env['DB_USERNAME'],\n",
        "                                         password=env['DB_PASSWORD'])\n",
        "\n",
        "#    connection = mysql.connector.connect(host=\"192.168.168.125\", database=\"requestdb\", user=\"requestuser\",\n",
        "                                         #password=\"SxfWL7myDid1JBqS\")\n",
        "except mysql.connector.Error as err:\n",
        "    print('There was an error connecting to the database!!!')\n",
        "    print(err)\n",
        "    exit()\n",
        "cursor = connection.cursor(dictionary=True)\n",
        "cursor.execute(query)\n",
        "last = cursor.fetchall()\n",
        "last = pd.DataFrame(last)\n",
        "# print (dfm)\n",
        "if connection.is_connected():\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "#removing unnecessary columns of the table--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "last = last.drop('statisticsType', axis=1)\n",
        "last = last.drop('subjectID', axis=1)\n",
        "last = last.drop('id', axis=1)\n",
        "last = last.drop('created_at', axis=1)\n",
        "last = last.drop('updated_at', axis=1)\n",
        "last.drop(last.tail(1).index,inplace=True)\n",
        "#_______________________________________________________________________________________________________\n",
        "last = last.drop('time', axis=1)\n",
        "print (last)\n",
        "\n",
        "data=last.copy()\n",
        "# Function to handle outliers/anomalies\n",
        "def iqr_fence(x):\n",
        "    Q1 = x.quantile(0.25)\n",
        "    Q3 = x.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    Lower_Fence = Q1 - (1.5 * IQR)\n",
        "    Upper_Fence = Q3 + (1.5* IQR)\n",
        "    u = max(x[x<Upper_Fence])\n",
        "    l = min(x[x>Lower_Fence])\n",
        "    return [u,l]\n",
        "\n",
        "\n",
        "def handle_outliers(data):\n",
        "\n",
        "    for column in data.columns:\n",
        "        threshold=iqr_fence(data[column])[0]\n",
        "        data[column] = data[column].mask(data[column] > threshold, int(threshold))\n",
        "    return data\n",
        "\n",
        "# Handle outliers/anomalies in the dataset\n",
        "data = handle_outliers(data)\n",
        "print(data)\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data.values)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "# Store the scaling parameters for inverse scaling of predictions\n",
        "scaler_params = {'min': scaler.data_min_[0], 'max': scaler.data_max_[0]}\n",
        "\n",
        "# Define the look-back values to try\n",
        "look_backs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "# Create the training data set\n",
        "def create_dataset(data, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(look_back, len(data)):\n",
        "        X.append(data[i - look_back:i, :])\n",
        "        y.append(data[i, :])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define the function to create the LSTM model\n",
        "def create_model(params):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=params['lstm_units'], return_sequences=True, input_shape=(params['look_back'], 1)))  # Update input_shape\n",
        "    model.add(LSTM(units=params['lstm_units']))\n",
        "    model.add(Dense(units=1))  # Update units\n",
        "\n",
        "    model.compile(optimizer=params['optimizer'], loss=params['loss_function'])\n",
        "    return model\n",
        "\n",
        "# Define the objective function for Hyperopt\n",
        "def objective(params):\n",
        "    model = create_model(params)\n",
        "\n",
        "    X_train, y_train = create_dataset(train_data, params['look_back'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    X_test, y_test = create_dataset(test_data, params['look_back'])\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = predictions * (scaler_params['max'] - scaler_params['min']) + scaler_params['min']\n",
        "\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    return {'loss': mse, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for Hyperopt\n",
        "space = {\n",
        "    'lstm_units': hp.choice('lstm_units', [50, 500]),\n",
        "    'look_back': hp.choice('look_back', look_backs),\n",
        "    'batch_size': hp.choice('batch_size', [1, 10]),\n",
        "    'epochs': hp.choice('epochs', [1, 400]),\n",
        "    'optimizer': hp.choice('optimizer', ['adam', 'sgd', 'rmsprop', 'adagrad', 'adamax']),\n",
        "    'loss_function': hp.choice('loss_function', ['mean_squared_error', 'binary_crossentropy', 'mean_absolute_error', 'huber_loss'])\n",
        "}\n",
        "\n",
        "# Create a list to store the best hyperparameters for each region\n",
        "best_hyperparameters = []\n",
        "\n",
        "# Define the function to create the LSTM model\n",
        "def create_model(params):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=params['lstm_units'], return_sequences=True, input_shape=(params['look_back'], 1)))  # Update input_shape\n",
        "    model.add(LSTM(units=params['lstm_units']))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(optimizer=params['optimizer'], loss=params['loss_function'])\n",
        "    return model\n",
        "\n",
        "MAE_Percentage_Error=[]\n",
        "Prediction_Metrice=[]\n",
        "best_look_backs = []\n",
        "# Iterate over each region\n",
        "for i, region in enumerate(data.columns):\n",
        "    print(\"Optimizing hyperparameters for region:\", region)\n",
        "\n",
        "    # Extract the data for the current region\n",
        "    region_data = data[[region]].values\n",
        "\n",
        "    # Preprocess the data\n",
        "    # scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # scaled_data = scaler.fit_transform(region_data)\n",
        "    scaled_data = region_data\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_size = int(len(scaled_data) * 0.8)\n",
        "    train_data = scaled_data[:train_size]\n",
        "    test_data = scaled_data[train_size:]\n",
        "\n",
        "    # Run Hyperopt optimization\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=40, trials=trials)\n",
        "\n",
        "    # Get the best hyperparameters for the current region\n",
        "    best_params = space_eval(space, best)\n",
        "    best_hyperparameters.append(best_params)\n",
        "\n",
        "    # Store the best look_back value for the current region\n",
        "    print(best_params['look_back'])\n",
        "    best_look_back = best_params['look_back']\n",
        "    best_look_backs.append(best_look_back)\n",
        "\n",
        "    # Train the model with the best hyperparameters for the current region\n",
        "    model = create_model(best_params)\n",
        "    X_train, y_train = create_dataset(train_data, best_params['look_back'])\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    X_test, y_test = create_dataset(test_data, best_params['look_back'])\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = predictions * (scaler_params['max'] - scaler_params['min']) + scaler_params['min']\n",
        "    print(np.shape(X_test))\n",
        "\n",
        "    previous=data[[region]][-best_look_back:]\n",
        "    print(\"previous\", previous)\n",
        "    values1 = previous.values.reshape(-1,1)\n",
        "    values1 = values1.astype('float32')\n",
        "    scaler1 = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled1 = scaler.fit_transform(values1)\n",
        "    # scaled1\n",
        "    previous=np.array(scaled1)\n",
        "    previous = scaled1. reshape(1, best_look_back, 1)\n",
        "\n",
        "\n",
        "#using the compiled model to make prediction of next month value using last four months values as inputs------------------------------------------------------\n",
        "\n",
        "    Real_prediction = model.predict(previous)\n",
        "    Real_prediction = Real_prediction * (scaler_params['max'] - scaler_params['min']) + scaler_params['min']\n",
        "    print(\"Real_prediction\", Real_prediction)\n",
        "    Prediction_Metrice.append(int(Real_prediction))\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(\"Evaluation metrics for region\", region)\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R2 Score:\", r2)\n",
        "    print()\n",
        "\n",
        "    MAE_Percentage_Error.append(mae*100/np.mean(data[region][-4:]))\n",
        "    # Store the predicted versus actual values in a DataFrame\n",
        "    predicted_table = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': predictions.flatten()})\n",
        "    print(\"Predicted versus actual values for region\", region)\n",
        "    print(predicted_table)\n",
        "    print()\n",
        "\n",
        "# Print the best hyperparameters for each region\n",
        "print(\"Best hyperparameters for each region:\")\n",
        "for i, region in enumerate(data.columns):\n",
        "    print(\"Region:\", region)\n",
        "    print(best_hyperparameters[i])\n",
        "    print(\"Best look_back:\", best_look_backs[i])\n",
        "    print()\n",
        "\n",
        "print(\"Prediction_Metrice:\", Prediction_Metrice)\n",
        "print(\"MAE_Percentage_Error:\", MAE_Percentage_Error)\n",
        "\n"
      ]
    }
  ]
}